import streamlit as st
import requests
import time
from scrape1 import scrape_website, extract_body_content, clean_body_content, split_dom_content
from parse import parse_with_ollama

st.set_page_config(page_title="AI Web Scraper", page_icon="ğŸŒ", layout="wide")

st.title("ğŸŒ AI Web Scraper")
st.write("Enter a website URL and get its cleaned text content. Perfect for quick insights!")

if "dom_content" not in st.session_state:
    st.session_state["dom_content"] = None

url = st.text_input("ğŸ”— Enter Website URL:", placeholder="https://example.com")


if st.button("ğŸš€ Start Scraping"):
    if not url.strip():
        st.warning("âš ï¸ Please enter a valid URL before scraping.")
    else:
        with st.spinner("ğŸ”„ Scraping in progress... Please wait"):
            try:
                start_time = time.time() 
                response = requests.post("http://127.0.0.1:8000/scrape", json={"url": url})

                if response.status_code == 200:
                    data = response.json()
                    execution_time = round(time.time() - start_time, 2) 

                    st.session_state["dom_content"] = data["cleaned_content"]

                    # Success Message
                    st.success(f"âœ… Scraping completed in {execution_time} seconds!")

                    with st.expander("ğŸ“œ View Cleaned Content", expanded=True):
                        st.text_area("ğŸ“ Cleaned Text Content:", data["cleaned_content"], height=300)

                    with st.expander("ğŸ“„ View Content Chunks"):
                        for i, chunk in enumerate(data["content_chunks"], 1):
                            st.text_area(f"Chunk {i}:", chunk, height=100)

                    with st.expander("ğŸ›  Raw HTML (for debugging)"):
                        st.code(data["raw_html"], language="html")

                else:
                    st.error(f"âŒ Failed to scrape the website. Error Code: {response.status_code}")

            except requests.exceptions.ConnectionError:
                st.error("âš ï¸ Unable to connect to the scraping server. Ensure FastAPI is running.")
            except requests.exceptions.Timeout:
                st.error("â³ The request took too long. Try again later.")
            except Exception as e:
                st.error(f"âŒ An unexpected error occurred: {str(e)}")

if st.session_state["dom_content"]:
    st.subheader("ğŸ›  Parse Scraped Content")
    parse_description = st.text_area("Describe what you want to parse:", placeholder="Example: Extract all product details")

    if st.button("ğŸ” Parse Content"):
        if parse_description.strip():
            with st.spinner("â³ Parsing content..."):

                dom_chunks = split_dom_content(st.session_state["dom_content"])
                result = parse_with_ollama(dom_chunks, parse_description)
                st.write(result)
                st.success("âœ… Parsing completed!")
        else:
            st.warning("âš ï¸ Please enter a description for parsing.")

# Footer
st.markdown("---")
st.caption("ğŸ” Built with Streamlit & FastAPI | Developed by Jay ğŸ’¡")
